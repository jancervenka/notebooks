{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6ODL-Q50vdYl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BestOfNReasoner:\n",
        "    def __init__(self, model_id: str):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=self.device\n",
        "        )\n",
        "        # Ensure pad token is set for batch generation\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def generate_candidates(self, prompt: str, n: int = 3):\n",
        "        \"\"\"\n",
        "        The Generator: Creates N distinct chains of thought using high temperature.\n",
        "        \"\"\"\n",
        "        print(f\"--- Generating {n} candidates ---\")\n",
        "\n",
        "        system_prompt = (\n",
        "            \"You are a deep reasoning AI. You must NOT answer immediately. \"\n",
        "            \"First, use a <think></think> block to break down the problem, check for edge cases, \"\n",
        "            \"and verify your logic step-by-step. \"\n",
        "            \"Only after thinking should you provide the final answer (after the ending </think> tag).\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        input_ids = (\n",
        "            self\n",
        "            .tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
        "            .input_ids\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        forced_prefix = \"<think>\\n\"\n",
        "        prefix_ids = (\n",
        "            self\n",
        "            .tokenizer(forced_prefix, add_special_tokens=False, return_tensors=\"pt\")\n",
        "            .input_ids\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        input_ids = torch.cat([input_ids, prefix_ids], dim=1)\n",
        "\n",
        "        # num_return_sequences=n generates N variations in parallel\n",
        "        outputs = self.model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.9,      # High temp to ensure diversity in reasoning\n",
        "            do_sample=True,\n",
        "            num_return_sequences=n,\n",
        "            pad_token_id=self.tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        candidates = []\n",
        "        for out in outputs:\n",
        "            # Decode and reconstruct the full text including the forced <think>\n",
        "            text = self.tokenizer.decode(out, skip_special_tokens=True)\n",
        "            candidates.append(text)\n",
        "\n",
        "        return candidates\n",
        "\n",
        "    def score_candidate(self, problem, candidate):\n",
        "        judge_prompt = (\n",
        "            f\"Review the following problem and proposed solution.\\n\"\n",
        "            f\"Problem: {problem}\\n\"\n",
        "            f\"Proposed Solution:\\n{candidate}\\n\\n\"\n",
        "            f\"Critique the logic. Look for fallacies. Then, on the very last line, \"\n",
        "            f\"output a score from 0 to 10. Format: 'Score: X'\\n\"\n",
        "        )\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": judge_prompt}]\n",
        "        inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(self.device)\n",
        "\n",
        "        # Low temperature for the judge (we want consistent, critical evaluation)\n",
        "        outputs = self.model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.1,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        critique = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "        # Simple parsing to find \"Score: X\"\n",
        "        try:\n",
        "            # Look for the last occurrence of \"Score:\"\n",
        "            score_text = critique.split(\"Score:\")[-1].strip()\n",
        "            # Take the first number found\n",
        "            score = int(''.join(filter(str.isdigit, score_text.split()[0])))\n",
        "        except:\n",
        "            score = 0 # Fallback if parsing fails\n",
        "\n",
        "        return score, critique\n",
        "\n",
        "    def solve(self, problem, n=3):\n",
        "        # 1. Generate N solutions\n",
        "        candidates = self.generate_candidates(problem, n)\n",
        "\n",
        "        scored_results = []\n",
        "\n",
        "        # 2. Critique each solution\n",
        "        for i, cand in enumerate(candidates):\n",
        "            score, critique = self.score_candidate(problem, cand)\n",
        "            print(f\"\\n[Candidate {i+1}] Score: {score}/10\")\n",
        "            print(f\"Critique snippet: {critique[:100]}...\") # Print start of critique\n",
        "            scored_results.append((score, cand, critique))\n",
        "\n",
        "        # 3. Select Best\n",
        "        # Sort by score descending\n",
        "        scored_results.sort(key=lambda x: x[0], reverse=True)\n",
        "        best_result = scored_results[0]\n",
        "\n",
        "        return best_result"
      ],
      "metadata": {
        "id": "KsZpYBBywkFp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "reasoner = BestOfNReasoner(model_id)\n",
        "\n",
        "problem = (\n",
        "    \"I have 3 apples today. Yesterday I ate one apple. \"\n",
        "    \"How many apples do I have now?\"\n",
        ")\n",
        "\n",
        "best_score, best_text, best_critique = reasoner.solve(problem, n=3)\n",
        "\n",
        "print(f\"\\n{'='*20} WINNING SOLUTION (Score {best_score}) {'='*20}\")\n",
        "# Extract just the answer part for cleanliness\n",
        "if \"</think>\" in best_text:\n",
        "    print(best_text.split(\"</think>\")[-1].strip())\n",
        "else:\n",
        "    print(best_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDZh4FqGy3t1",
        "outputId": "37982aa6-6336-44ec-dd91-f23f260cbfdf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating 3 candidates ---\n",
            "\n",
            "[Candidate 1] Score: 8/10\n",
            "Critique snippet: Score: 8\n",
            "\n",
            "The logic provided is correct but could be more explicit about the process. The breakdown ...\n",
            "\n",
            "[Candidate 2] Score: 8/10\n",
            "Critique snippet: Score: 8\n",
            "\n",
            "Explanation:\n",
            "- The logic provided correctly accounts for the fact that eating an apple doe...\n",
            "\n",
            "[Candidate 3] Score: 8/10\n",
            "Critique snippet: Score: 8\n",
            "\n",
            "The logic provided correctly accounts for the fact that you did not eat an apple yesterday...\n",
            "\n",
            "==================== WINNING SOLUTION (Score 8) ====================\n",
            "Therefore, the answer is 2 apples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76cSKpn21yvc",
        "outputId": "9a6a5250-d222-434c-9f30-c782f089e572"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "You are a deep reasoning AI. You must NOT answer immediately. First, use a <think></think> block to break down the problem, check for edge cases, and verify your logic step-by-step. Only after thinking should you provide the final answer (after the ending </think> tag).\n",
            "user\n",
            "I have 3 apples today. Yesterday I ate one apple. How many apples do I have now?\n",
            "assistant\n",
            "<think>\n",
            "To solve this, let's break it down:\n",
            "1. Today: We start with 3 apples.\n",
            "2. Yesterday: We consumed 1 apple.\n",
            "3. Now we need to find out how many apples we have left.\n",
            "\n",
            "Calculation:\n",
            "- Start with 3 apples.\n",
            "- Subtract the 1 apple that was eaten yesterday.\n",
            "So, 3 - 1 = 2\n",
            "\n",
            "We don't eat or lose any apples in between, so the number of apples remains unchanged from today.\n",
            "</think>\n",
            "\n",
            "Therefore, the answer is 2 apples.\n"
          ]
        }
      ]
    }
  ]
}