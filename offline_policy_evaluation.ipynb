{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "75c9cc36-6b7c-44f1-a542-19e9e77ac6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025685b3-8de0-40a3-80e7-4a8a9ae389d5",
   "metadata": {},
   "source": [
    "# Offline Policy Evaluation\n",
    "Based on: https://github.com/banditml/offline-policy-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "74a99542-ed6d-48f3-bed2-9dc3764385a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_notification_policy(last_active):\n",
    "    \"\"\"\n",
    "    Send a notification with 0.9 probability if a user was\n",
    "    last active more than 1 days ago.\n",
    "\n",
    "    0.1 probability for exploration.\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 0.1\n",
    "    return 1 - epsilon if last_active > 1 else epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "069599fb-1330-4d64-9706-b9ce9d4e6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(policy):\n",
    "    \"\"\"\n",
    "    Generate an action using a `policy` and compute the reward\n",
    "    based on the action and user response.\n",
    "    \n",
    "    reward = 10 if a user logged in after receiveing _no_ notification\n",
    "    reward = 7 if a user logged in after being notified (we penalize notifications)\n",
    "    reward = 0 if not logged in regardless of the action\n",
    "    \"\"\"\n",
    "    \n",
    "    last_active = int(round(np.random.exponential(3)))\n",
    "    send_prob = policy(last_active)\n",
    "    action = \"send\" if send_prob > np.random.rand() else \"dont_send\"\n",
    "    \n",
    "    active_after_action = np.random.rand() < 0.78\n",
    "    match (active_after_action, action):\n",
    "        case (True, \"dont_send\"):\n",
    "            reward = 10\n",
    "        case (True, \"send\"):\n",
    "            reward = 7\n",
    "        case (False, _):\n",
    "            reward = 0\n",
    "    \n",
    "    return {\n",
    "        \"last_active\": last_active,\n",
    "        \"send_prob\": send_prob,\n",
    "        \"active_after_action\": active_after_action,\n",
    "        \"action\": action,\n",
    "        \"reward\": reward\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_samples(policy, n=1000):\n",
    "    np.random.seed()\n",
    "    return pd.DataFrame(generate_sample(policy) for _ in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3827777e-1924-46f8-a763-d3dc52bfea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_samples(current_notification_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "13f21234-a203-456f-a070-c4d0cfb78dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(\n",
    "    {\n",
    "        \"last_active\": [0, 0, 1, 2, 5, 7, 8, 10, 14],\n",
    "        \"send_prob\": [.1, .1, .1, .9, .9, .9, .9, .9, .9],\n",
    "        \"action\": [\"dont_send\"] * 4 + [\"send\", \"dont_send\"] * 2 + [\"send\"],\n",
    "        \"reward\": [10, 10, 10, 10, 7, 0, 7, 0, 7]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366518a-3d73-4b2c-80ee-9d7dacf95e25",
   "metadata": {},
   "source": [
    "### Inverse Propensity Scoring (IPS)\n",
    "\n",
    "IPS computes an action distribution mismatch between the new policy and the\n",
    "logged policy that generated the historical data. It is defined as\n",
    "\n",
    "$$\\hat{V} = \\frac{1}{N}\\sum_{k=1}^{N}\\frac{\\nu(a_k \\mid x_k)}{\\hat{\\mu}_k(a_k \\mid x_k)}r_k$$\n",
    "\n",
    "where $\\hat{V}$ is the expected average reward per decision under the new policy, $N$ is the\n",
    "number of logged historical samples, $\\nu(a_k \\mid x_k)$ is the probability that the new policy\n",
    "takes the logged action $a_k$ given the logged context $x_k$, $\\hat{\\mu}_k(a_k \\mid x_k)$ is the\n",
    "probability that your logged policy takes the logged action $a_k$ given the logged context $x_k$,\n",
    "and $r_k$ is the logged reward for the given historical sample $k$.\n",
    "\n",
    "For samples with high reward $r_k$, we want $\\nu$ to be much larger than $\\mu_k$. This means that\n",
    "our new policy works better than the logged policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "cbc9febb-434b-4252-8252-4c8a4815bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_good_notification_policy(last_active):\n",
    "\n",
    "    epsilon = 0.1\n",
    "    return 1 - epsilon if last_active > 5 else epsilon\n",
    "\n",
    "def new_bad_notification_policy(last_active):\n",
    "\n",
    "    epsilon = 0.1\n",
    "    return 1 - epsilon if last_active < 5 else epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f19b1b18-657d-404a-893c-fecff5ecc87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ips(sample, policy, no_reward_weight=False):\n",
    "    \"\"\"\n",
    "    Computes IPS for one sample.\n",
    "    \"\"\"\n",
    "\n",
    "    send_prob = sample[\"send_prob\"]\n",
    "    # last_active := our context\n",
    "    last_active = sample[\"last_active\"]\n",
    "\n",
    "    if sample[\"action\"] == \"send\":\n",
    "        mu = send_prob\n",
    "        nu = policy(last_active)\n",
    "    else:\n",
    "        mu = 1 - send_prob\n",
    "        nu = 1 - policy(last_active)\n",
    "\n",
    "    return (nu / mu) * (1 if no_reward_weight else sample[\"reward\"])\n",
    "\n",
    "\n",
    "def inverse_propensity_scoring(data, policy):\n",
    "    \"\"\"\n",
    "    Computes total IPS.\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(data)\n",
    "    _sample_ips = partial(sample_ips, policy=policy, no_reward_weight=False)\n",
    "    return data.apply(_sample_ips, axis=1).sum() / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4608c359-b269-4465-9884-412999485470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(scoring, data):\n",
    "    \n",
    "    policies = {\n",
    "        \"current\": current_notification_policy,\n",
    "        \"new_good\": new_good_notification_policy,\n",
    "        \"new_bad\": new_bad_notification_policy\n",
    "    }\n",
    "    \n",
    "    for name, policy in policies.items():\n",
    "        print(name, f\"score={scoring(data, policy):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "0ec5305c-70ef-4d71-b11c-e19fe24cc440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current score=6.78\n",
      "new_good score=14.98\n",
      "new_bad score=1.74\n"
     ]
    }
   ],
   "source": [
    "# inverse_propensity_scoring(data, current_notification_policy)\n",
    "# equals to data[\"reward\"].sum() / len(data)\n",
    "# because every nu / mu = 1\n",
    "\n",
    "evaluate(inverse_propensity_scoring, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07441d-2e3e-45c2-b63f-44344e540655",
   "metadata": {},
   "source": [
    "### Direct Method (DM)\n",
    "\n",
    "DM uses a model predicting the reward $\\hat{r}$ based on a action and context.\n",
    "\n",
    "$$\\hat{V} = \\frac{1}{N} \\sum_{k=1}^{N} \\sum_{a \\in A} \\nu(a \\mid x_k) \\hat{r}(x_k, a)$$\n",
    "\n",
    "where $\\hat{V}$ is the expected average reward per decision under the new policy, $N$ is the\n",
    "number of logged historical samples, $\\nu(a \\mid x_k)$ is the probability that the new policy\n",
    "takes an action $a$ given the logged context $x_k$, $\\hat{r}(x_k, a)$ is the predicted\n",
    "reward using the context $x_k$ of sample $k$ an action $a$ and $A$ is the set of all posible actions.\n",
    "\n",
    "We need a model beacuse for each logged context $x_k$, we want to predict the reward of every possible action\n",
    "$a \\in A$ and we only have one action $a_k$ logged.\n",
    "\n",
    "For an action with high predicted reward $\\hat{r}(x_k, a)$, we want large probability $\\nu(a \\mid x_k)$ of the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "109b9b7a-40e2-42a3-934b-52ec46f3a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_model_factory(data):    \n",
    "    # handle categorical \"action\" feature\n",
    "    X = pd.get_dummies(\n",
    "        data=data[[\"last_active\", \"action\"]],\n",
    "        columns=[\"action\"], \n",
    "        drop_first=True\n",
    "    ).values\n",
    "    y = data[\"reward\"].values\n",
    "    \n",
    "    model = GradientBoostingRegressor(random_state=0)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    print(f\"MSE={mean_squared_error(y, model.predict(X)):.2f}\")\n",
    "    print(f\"MAE={median_absolute_error(y, model.predict(X)):.2f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def sample_dm(sample, policy, reward_model):\n",
    "    \"\"\"\n",
    "    Computes DM for one sample.\n",
    "    \"\"\"\n",
    "    last_active = sample[\"last_active\"]\n",
    "\n",
    "    # r_hat for both actions (send, dont_send), same context\n",
    "    # 1 := send, 0 := dont_send\n",
    "    r_hats = reward_model.predict(\n",
    "        [\n",
    "            [last_active, 1],\n",
    "            [last_active, 0]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    send_prob = policy(last_active)\n",
    "    # nu for both actions, same context (last_active is same for both)\n",
    "    nus = np.array([send_prob, 1 - send_prob])\n",
    "\n",
    "    # inner sum\n",
    "    return np.dot(nus, r_hats)\n",
    "\n",
    "\n",
    "def direct_method(data, policy, reward_model):\n",
    "    \"\"\"\n",
    "    Computes total DM.\n",
    "    \"\"\"\n",
    "\n",
    "    # outer sum    \n",
    "    N = len(data)\n",
    "    _sample_dm = partial(sample_dm, policy=policy, reward_model=reward_model)\n",
    "    return data.apply(_sample_dm, axis=1).sum() / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b1c8cb55-e17e-413b-ac77-686802f9a1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE=11.82\n",
      "MAE=2.23\n"
     ]
    }
   ],
   "source": [
    "reward_model = reward_model_factory(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "802fcb88-571e-41b8-8dbd-858f162827bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current score=6.21\n",
      "new_good score=6.74\n",
      "new_bad score=5.73\n"
     ]
    }
   ],
   "source": [
    "evaluate(partial(direct_method, reward_model=reward_model), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d71489-32cd-4891-89d7-f935f5626262",
   "metadata": {},
   "source": [
    "### Doubly Robust Method (DRM)\n",
    "\n",
    "Doubly robust method combines DM and IPS and is a strict improvement over both methos.\n",
    "It is defined as\n",
    "\n",
    "$$\n",
    "    \\hat{V} = \\frac{1}{N} \\sum_{k=1}^{N}\n",
    "    \\left(\n",
    "    \\sum_{a \\in A} \\nu(a \\mid x_k) \\hat{r}(x_k, a)\n",
    "    + \\frac{\\nu(a_k \\mid x_k)}{\\hat{\\mu}_k(a_k \\mid x_k)} (r_k - \\hat{r}(x_k, a_k))\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "We can see from the formula that the method is using DM and then applies a correction based on IPS.\n",
    "If the reward predicted by the model sample $k$ is equal to the logged reward $r_k$ then\n",
    "\n",
    "$$\n",
    "(r_k - \\hat{r}(x_k, a_k)) = 0\n",
    "$$\n",
    "\n",
    "and the result is equal to just the direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "bc12ad61-6fc7-404a-93c9-3095dbe63f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_drm(sample, policy, reward_model):\n",
    "    \"\"\"\n",
    "    Computes DRM for one sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    last_active = sample[\"last_active\"]\n",
    "    is_send = sample[\"action\"] == \"send\"\n",
    "\n",
    "    dm = sample_dm(sample, policy, reward_model)\n",
    "    raw_ips = sample_ips(sample, policy, no_reward_weight=True)\n",
    "    \n",
    "    r = sample[\"reward\"]\n",
    "    r_hat = reward_model.predict([[last_active, int(is_send)]])[0]\n",
    "    ips_weight = r - r_hat\n",
    "    \n",
    "    return dm + raw_ips * ips_weight\n",
    "\n",
    "\n",
    "def doubly_robust_method(data, policy, reward_model):\n",
    "    \"\"\"\n",
    "    Computes total DRM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # outer sum    \n",
    "    N = len(data)\n",
    "    _sample_drm = partial(sample_drm, policy=policy, reward_model=reward_model)\n",
    "    return data.apply(_sample_drm, axis=1).sum() / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "85c3cbf8-592d-4de5-8ca2-49c5a34b9095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current score=6.26\n",
      "new_good score=8.46\n",
      "new_bad score=-7.47\n"
     ]
    }
   ],
   "source": [
    "evaluate(partial(doubly_robust_method, reward_model=reward_model), test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
