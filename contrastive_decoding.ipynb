{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea72206d-a9ba-4018-bd32-901c03157345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T22:20:42.064387Z",
     "iopub.status.busy": "2025-03-16T22:20:42.064102Z",
     "iopub.status.idle": "2025-03-16T22:20:55.514956Z",
     "shell.execute_reply": "2025-03-16T22:20:55.513763Z",
     "shell.execute_reply.started": "2025-03-16T22:20:42.064365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.35.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.0\n",
      "    Uninstalling safetensors-0.4.0:\n",
      "      Successfully uninstalled safetensors-0.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.1\n",
      "    Uninstalling tokenizers-0.15.1:\n",
      "      Successfully uninstalled tokenizers-0.15.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed huggingface-hub-0.29.3 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.49.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9bf8e1-165c-4b85-a461-15d334136b4b",
   "metadata": {},
   "source": [
    "Paper: https://arxiv.org/pdf/2210.15097\n",
    "\n",
    "Let $x_{\\text{pre}} = x_1, \\ldots, x_n$ denote a prompt of $n$ tokens $x_i \\in \\mathcal{V}$ from a vocabulary $\\mathcal{V}$\n",
    "and $x_{\\text{cont}} = x_{n+1}, \\ldots, x_{n + m}$ denotes the output generated from the prompt. Autoregressive language\n",
    "model decodes a single token at a time wih a next token probability distribution $p_{\\text{LM}}(x_i|x_{<i})$\n",
    "\n",
    "$$\n",
    "p_{\\text{LM}}(x_\\text{cont}|x_{\\text{pre}}) = \\prod_{i=n+1}^{n+m} p_{\\text{LM}}(x_i|x_{<i})\n",
    "$$\n",
    "\n",
    "Small models (AMA) give higher probability to tokens producing undesirable patterns (e.g. repetitions) than larger\n",
    "models (EXP). Contrastive decoding combines a small and a larger model using an objective function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{CD}}(x_\\text{cont}, x_{\\text{pre}}) = \\log p_{\\text{EXP}}(x_\\text{cont}|x_{\\text{pre}})\n",
    "- \\log p_{\\text{AMA}}(x_\\text{cont}|x_{\\text{pre}})\n",
    "$$\n",
    "\n",
    "that penalizes patterns favored by the smaller model while rewarding patterns favored by the larger model.\n",
    "This should improve the overall output. The continuation tokens $x_\\text{cont}$ are picked from a truncated\n",
    "vocabulary $\\mathcal{V}_{\\text{head}}(x_{<i})$ defined as \n",
    "\n",
    "$$\n",
    "\\mathcal{V}_{\\text{head}}(x_{<i}) = \n",
    "\\{x_i \\in \\mathcal{V} : p_{\\text{EXP}}(x_i|x_{<i}) > \\alpha \\max_{w} p_{\\text{EXP}}(w|x_{<i}) \\}\n",
    "$$\n",
    "\n",
    "where $\\alpha = 0.1$. The truncated vocabulary at the $i$-th step contains only tokens which probability\n",
    "is higher than $\\alpha$ times the probability of the most probable token. The truncatation limits\n",
    "false positives (implausible tokens with large difference between $p_{\\text{EXP}}$ and $p_{\\text{AMA}}$)\n",
    "and false negatives (obviously correct tokens with high $p_{\\text{EXP}}$ and $p_{\\text{AMA}}$)\n",
    "by keeping a pool of only high probability candidates.\n",
    "\n",
    "Token-level $\\text{CD-score}$ is\n",
    "\n",
    "$$\n",
    "\\text{CD-score}(x_i, x_{<i}) = \\log \\frac{p_{\\text{EXP}}(x_i|x_{<i})}{p_{\\text{AMA}}(x_i|x_{<i})},\n",
    "\\forall x_i \\in \\mathcal{V}_{\\text{head}}(x_{<i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a807ca76-02cc-43f0-9deb-572f0338f374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T22:53:58.327297Z",
     "iopub.status.busy": "2025-03-16T22:53:58.326370Z",
     "iopub.status.idle": "2025-03-16T22:54:02.468832Z",
     "shell.execute_reply": "2025-03-16T22:54:02.468096Z",
     "shell.execute_reply.started": "2025-03-16T22:53:58.327253Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers as tr\n",
    "import torch\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "amateur_path = 'Qwen/Qwen2.5-Coder-0.5B-Instruct'\n",
    "expert_path = 'Qwen/Qwen2.5-Coder-1.5B-Instruct'\n",
    "# device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(amateur_path)\n",
    "amateur = tr.AutoModelForCausalLM.from_pretrained(amateur_path).to(device)\n",
    "expert = tr.AutoModelForCausalLM.from_pretrained(expert_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1717ed00-979f-414a-a002-8cb0c329fb0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T22:54:15.842043Z",
     "iopub.status.busy": "2025-03-16T22:54:15.841139Z",
     "iopub.status.idle": "2025-03-16T22:54:15.860555Z",
     "shell.execute_reply": "2025-03-16T22:54:15.859813Z",
     "shell.execute_reply.started": "2025-03-16T22:54:15.842006Z"
    }
   },
   "outputs": [],
   "source": [
    "user_message = \"\"\"Give a very very brief docstring for the following function:\\n```\\nfunction updateEloScores(\n",
    "\tscores,\n",
    "\tresults,\n",
    "\tkFactor = 4,\n",
    ") {\n",
    "\tfor (const result of results) {\n",
    "\t\tconst { first, second, outcome } = result;\n",
    "\t\tconst firstScore = scores[first] ?? 1000;\n",
    "\t\tconst secondScore = scores[second] ?? 1000;\n",
    "\n",
    "\t\tconst expectedScoreFirst = 1 / (1 + Math.pow(10, (secondScore - firstScore) / 400));\n",
    "\t\tconst expectedScoreSecond = 1 / (1 + Math.pow(10, (firstScore - secondScore) / 400));\n",
    "\t\tlet sa = 0.5;\n",
    "\t\tif (outcome === 1) {\n",
    "\t\t\tsa = 1;\n",
    "\t\t} else if (outcome === -1) {\n",
    "\t\t\tsa = 0;\n",
    "\t\t}\n",
    "\t\tscores[first] = firstScore + kFactor * (sa - expectedScoreFirst);\n",
    "\t\tscores[second] = secondScore + kFactor * (1 - sa - expectedScoreSecond);\n",
    "\t}\n",
    "\treturn scores;\n",
    "}\\n```\"\"\"\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant'},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da099b4b-182a-4be8-b7dc-62ce96a637a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T22:54:17.575066Z",
     "iopub.status.busy": "2025-03-16T22:54:17.574744Z",
     "iopub.status.idle": "2025-03-16T22:54:17.581806Z",
     "shell.execute_reply": "2025-03-16T22:54:17.580714Z",
     "shell.execute_reply.started": "2025-03-16T22:54:17.575042Z"
    }
   },
   "outputs": [],
   "source": [
    "def contrastive_decoding(amateur, expert, tokenizer, prompt, max_tokens=100, alpha=0.1):\n",
    "    \n",
    "    # tensor[int], shape (1, n_tokens)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    for _ in tqdm.tqdm(range(max_tokens)):\n",
    "        with torch.no_grad():\n",
    "            # next token scores (before softmax), tensor[float] shape(1, vocab_size)\n",
    "            amateur_logits = amateur(input_ids).logits[:, -1, :]\n",
    "            expert_logits = expert(input_ids).logits[:, -1, :]\n",
    "\n",
    "        # convert raw scores to log probabilities, shape(1, vocab_size)\n",
    "        amateur_log_probs = torch.log_softmax(amateur_logits, dim=-1)\n",
    "        expert_log_probs = torch.log_softmax(expert_logits, dim=-1)\n",
    "\n",
    "        # compute contrastive score, shape(1, vocab_size)\n",
    "        contrastive_scores = expert_log_probs - amateur_log_probs\n",
    "\n",
    "        # compute expert probabilities from expert raw scores, shape(1, vocab_size)\n",
    "        expert_probs = torch.softmax(expert_logits, dim=-1)\n",
    "        # vocabulary truncation treshold = alpha * probability of the most probable next token\n",
    "        threshold = alpha * expert_probs.max()\n",
    "        # find tokens with the expert probability below the truncation threshold\n",
    "        mask = expert_probs >= threshold\n",
    "        # truncate the next token vocabulary by setting the CD score to infinite\n",
    "        contrastive_scores[~mask] = float('-inf')\n",
    "\n",
    "        # Select the token with the highest CD score from the truncated vocabulary\n",
    "        best_token = torch.argmax(contrastive_scores, dim=-1)\n",
    "        # add the token to the input ids and go to the next iteration\n",
    "        input_ids = torch.cat([input_ids, best_token.unsqueeze(0)], dim=-1)\n",
    "\n",
    "        # Stop if end token is generated\n",
    "        if best_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf0f5bb-b79e-44fd-8520-8c22cd973953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T22:54:20.723232Z",
     "iopub.status.busy": "2025-03-16T22:54:20.722289Z",
     "iopub.status.idle": "2025-03-16T22:55:17.199781Z",
     "shell.execute_reply": "2025-03-16T22:55:17.197693Z",
     "shell.execute_reply.started": "2025-03-16T22:54:20.723202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e8cdaeafdc45a29e7c4f91280fb8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant\n",
      "user\n",
      "Give a very very brief docstring for the following function:\n",
      "```\n",
      "function updateEloScores(\n",
      "\tscores,\n",
      "\tresults,\n",
      "\tkFactor = 4,\n",
      ") {\n",
      "\tfor (const result of results) {\n",
      "\t\tconst { first, second, outcome } = result;\n",
      "\t\tconst firstScore = scores[first] ?? 1000;\n",
      "\t\tconst secondScore = scores[second] ?? 1000;\n",
      "\n",
      "\t\tconst expectedScoreFirst = 1 / (1 + Math.pow(10, (secondScore - firstScore) / 400));\n",
      "\t\tconst expectedScoreSecond = 1 / (1 + Math.pow(10, (firstScore - secondScore) / 400));\n",
      "\t\tlet sa = 0.5;\n",
      "\t\tif (outcome === 1) {\n",
      "\t\t\tsa = 1;\n",
      "\t\t} else if (outcome === -1) {\n",
      "\t\t\tsa = 0;\n",
      "\t\t}\n",
      "\t\tscores[first] = firstScore + kFactor * (sa - expectedScoreFirst);\n",
      "\t\tscores[second] = secondScore + kFactor * (1 - sa - expectedScoreSecond);\n",
      "\t}\n",
      "\treturn scores;\n",
      "}\n",
      "```\n",
      "assistant\n",
      "This function `updateEloScores` takes three\n"
     ]
    }
   ],
   "source": [
    "output = contrastive_decoding(amateur, expert, tokenizer, prompt, max_tokens=10)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
