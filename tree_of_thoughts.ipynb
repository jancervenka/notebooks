{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8437c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7e3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeOfThoughtsReasoner:\n",
    "    def __init__(self, model_id: str):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            device_map=self.device\n",
    "        )\n",
    "    \n",
    "    def query_model(\n",
    "        self, prompt: str, k: int, max_tokens: int = 200\n",
    "    ) -> list[str]:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ]\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        output = self.model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=max_tokens, \n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=k,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        # to remove the input prompt from the output\n",
    "        input_len = input_ids.shape[1]\n",
    "        # return a list of k outputs\n",
    "        return [\n",
    "            self.tokenizer.decode(out[input_len:], skip_special_tokens=True).strip()\n",
    "            for out in output\n",
    "        ]\n",
    "    \n",
    "    def get_next_steps(\n",
    "        self,\n",
    "        current_state: str,\n",
    "        task_description: str,\n",
    "        current_depth: int,\n",
    "        max_depth: int,\n",
    "        k: int = 3,\n",
    "    ) -> list[str]:\n",
    "        # given the current partial solution, generate k possible next steps\n",
    "        prompt = f\"\"\"Task: {task_description}\n",
    "Current partial solution: {current_state}\n",
    "Based on this, write the single most logical next step and its solution.\n",
    "There will be {max_depth} steps in total.\n",
    "Right now, you need to generate the step {current_depth}.\n",
    "Output ONLY the next logical step with its solution. Keep it brief.\"\"\"\n",
    "\n",
    "        possible_steps = self.query_model(prompt, max_tokens=100, k=k)\n",
    "        return [current_state + \"\\n\" + step for step in possible_steps]\n",
    "\n",
    "    def evaluate_state(\n",
    "        self, current_state: str, task_description: str\n",
    "    ) -> float:\n",
    "        # ask the model to score the current partial solution (0.1 to 1.0)\n",
    "        prompt = f\"\"\"Task: {task_description}\n",
    "Proposed partial solution: {current_state}\n",
    "Evaluate if this solution is on the right track to solving the task.\n",
    "Consider logical consistency and constraints.\n",
    "Give a score between 0.1 and 1.0 where 1.0 is perfect.\n",
    "Output ONLY the number.\"\"\"\n",
    "        \n",
    "        response = self.query_model(prompt, max_tokens=10, k=1)[0]\n",
    "        try:\n",
    "            # extract the first floating point number found in the text\n",
    "            score = float(re.findall(r\"0\\.\\d+|1\\.0|0\", response)[0])\n",
    "        except Exception:\n",
    "            score = 0.5\n",
    "            \n",
    "        return score\n",
    "\n",
    "    def solve(self, task: str, max_depth: int = 3, beam_width: int = 2) -> str:\n",
    "\n",
    "        # start with 1 empty state\n",
    "        current_beams = [\"\"] \n",
    "        \n",
    "        for current_depth in range(1, max_depth + 1):            \n",
    "            all_candidates = []\n",
    "\n",
    "            for beam in current_beams:\n",
    "                next_steps = self.get_next_steps(beam, task, current_depth, max_depth, k=3)\n",
    "                \n",
    "                for possible_step in next_steps:\n",
    "                    score = self.evaluate_state(possible_step, task)\n",
    "                    all_candidates.append({\"score\": score, \"step\": possible_step})\n",
    "\n",
    "            # keep only the top `beam_width` candidates for the next depth level\n",
    "            all_candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "            current_beams = [candidate[\"step\"] for candidate in all_candidates[:beam_width]]\n",
    "\n",
    "        return current_beams[0] # Return the single best path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92ce8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f028bf6795428a99a7a20340f8e552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b150aeaef1bb4c178a824f5dc153ca7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524e79f40066417194abb3a4159d02a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a56c79aa74b460f9364fa9a75832a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0137a16a86344ef8985554091762cead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac5453976014216b4e1b0674fae011b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dc2fc12856465a9fb0ed88b380c34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tot_engine = TreeOfThoughtsReasoner(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "460f5299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next logical step:\n",
      "(2 + 1) = ?\n",
      "\n",
      "Solution:\n",
      "3\n",
      "Step 2:\n",
      "3 * 3 = ?\n",
      "3 * 3 = 9\n"
     ]
    }
   ],
   "source": [
    "problem = \"Calculate (2 + 1) * 3\"\n",
    "solution = tot_engine.solve(problem, max_depth=3, beam_width=2)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14329190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_problem() -> tuple[str, int]:\n",
    "    a, b, c = np.random.randint(0, 100, 3)\n",
    "    return f\"Calculate ({a:.0f} + {b:.0f}) * {c:.0f}. Output just the final number\", (a + b) * c\n",
    "\n",
    "\n",
    "def extract_result(response) -> int:\n",
    "    try:\n",
    "        return int(re.findall(r\"\\d+\", response)[-1])\n",
    "    except Exception:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f376f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "problems, targets = zip(*[get_problem() for _ in range(30)])\n",
    "\n",
    "answers_simple = [tot_engine.query_model(p, k=1, max_tokens=50)[0] for p in problems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65007494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e809d56c1546abb425d3301e7829de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers_tot = []\n",
    "for problem in tqdm.tqdm(problems):\n",
    "    answers_tot.append(tot_engine.solve(problem, max_depth=3, beam_width=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32f81cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(targets)\n",
    "extracted_answers_simple = np.array([extract_result(ans) for ans in answers_simple])\n",
    "extracted_answers_tot = np.array([extract_result(ans) for ans in answers_tot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f2d8bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (base model): 0.06666666666666667\n",
      "Accuracy (tree of thoughts): 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Accuracy (base model):\", (targets == extracted_answers_simple).mean()\n",
    ")\n",
    "print(\n",
    "    \"Accuracy (tree of thoughts):\", (targets == extracted_answers_tot).mean()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
