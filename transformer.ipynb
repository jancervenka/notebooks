{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59e56dcf-1ee5-4bc6-b237-99874b77fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438d4a0-5de4-46b8-b615-76d8c2081ee3",
   "metadata": {},
   "source": [
    "[1]: https://twitter.com/abhi1thakur/status/1470406495678439426\n",
    "[2]: https://arxiv.org/pdf/2207.09238.pdf\n",
    "\n",
    "### Attention is all you need\n",
    "1. [Thread][1]\n",
    "2. [Paper][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5938f-945a-417b-8030-7992db6322b1",
   "metadata": {},
   "source": [
    "#### Basic Building Blocks\n",
    "\n",
    "This is the `EDTransformer` (encoder-decoder) architecture as described in [2] used for seq2seq inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792dd04-5756-4bb3-bd3c-65e3431289ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Unsused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f84d612-45cb-4369-9e5f-56d8dc3958c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing embedding\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=512,\n",
    "        num_heads=8,  # hyperparam H in [2]\n",
    "        num_encoders=6,  # hyperparam l_enc in [2]\n",
    "        num_decoders=6,  # hyperparam l_dec in [2]\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(d_model, num_heads, num_encoders)\n",
    "        self.decoder = Decoder(d_model, num_heads, num_decoders)\n",
    "    \n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        # source, target = Z, X in [2] (in case of EDTransformer)\n",
    "        # source = Z, target = X (context in the decoder)\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        return self.decoder(target, encoder_output, source_mask, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5304f40e-43d3-4c30-8970-879dbda6a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_decoders):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(d_model, num_heads)\n",
    "                for _ in range(num_decoders)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, target, encoder, target_mask, encoder_mask):\n",
    "        # Decoder takes the output from last the encoder layer\n",
    "        # along with target embeddings and target mask as its input.\n",
    "        # Encoder mask is still the source mask.\n",
    "        output = None\n",
    "        for layer in self.decoder_layers:\n",
    "            output = layer(\n",
    "                output if output is not None else target, encoder, target_mask, encoder_mask\n",
    "            )\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.3):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # This multi-head attention takes key, value (K, V) from\n",
    "        # the final encoder output (K, V are equal). Query\n",
    "        # comes from the output of the masked multi-head attention\n",
    "        self.masked_attention = MultiHeadAttention(\n",
    "            d_model, num_heads, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_model, num_heads, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # feed forward part, same as the encoder\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.attention_norm = nn.LayerNorm(d_model)\n",
    "        self.masked_attention_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, target, encoder, target_mask, encoder_mask):\n",
    "        x = target\n",
    "        # line 14 of alg 8 from [2], q, k, v == target == X\n",
    "        # Target mask := [[t_z <= t_x]] only tokens preceeding the\n",
    "        # current token are used as the context (unidirectional/masked self-attention).\n",
    "        x = self.masked_attention(q=x, k=x, v=x, mask=target_mask)\n",
    "        x = self.masked_attention_norm(x)\n",
    "        # Query comes from the output of the masked attention.\n",
    "        # key, value come from the last encoder layer.\n",
    "        # line 16 of alg 8 from [2]\n",
    "        # q is X (target), k, v is the source (context) Z encoded by the encoder\n",
    "        x = x + self.attention(q=x, k=encoder, v=encoder, mask=encoder_mask)\n",
    "        x = self.attention_norm(x)\n",
    "        x = x + self.ffn(x)\n",
    "        x = self.ffn(norm)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014fc37-f9f6-4773-bd5d-ee54949b2763",
   "metadata": {},
   "source": [
    "#### Used for ETransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebc7df85-3d49-46ba-83d6-6df36593ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, num_encoders):\n",
    "        # Encoder consists of `num_encoder` layers (L_enc in [2]).\n",
    "        # ModuleList holds a list of submodules, can be indexed as Python list.\n",
    "        # `linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])`\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(d_model, num_heads)\n",
    "                for _ in range(num_encoders)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, source, source_mask):\n",
    "        output = None\n",
    "        # source shape (batch_size, sequence_len, embedding_dim)\n",
    "        # source is the tensor of embedded tokens\n",
    "        for layer in self.encoder_layers:\n",
    "            # The output of one encoder goes into the next one.\n",
    "            # Source mask stays the same.\n",
    "            output = layer(output if output is not None else source, source_mask)\n",
    "        # output shape (batch_size, sequence_len, d_model == embedding_dim)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "795d59b9-3e27-4bed-8ad4-8fb5b6e16b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    # layer l\n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.3):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # params \\mathcal{W}_l^enc from [2] (H heads)\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_model, num_heads, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # feed forward part of the encoder, after attention\n",
    "        self.ffn = nn.Sequential(\n",
    "            # Linear (Wx + b) is the same as Keras Dense layer without activations.\n",
    "            nn.Linear(d_model, d_ff),  # W_mlp1^l, b_mlp2^l from [2]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),  # W_mlp2^l, b_mlp2^l from [2]\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        self.attention_norm = nn.LayerNorm(d_model)  # β_l^1, γ_l^1 norm parameters\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)  # β_l^1, γ_l^1 norm parameters\n",
    "        \n",
    "    def forward(self, source, source_mask):\n",
    "        x = source\n",
    "        # x shape (batch_size, sequence_len, d_model)\n",
    "        x = x + self.attention(q=x, k=x, v=x, mask=source_mask)  # line 5 of alg 8 from [2]\n",
    "        x = self.attention_norm(x)  # line 6\n",
    "        # x shape (batch_size, sequence_len, d_model)\n",
    "        x = x + self.ffn(x)  # line 7\n",
    "        x = self.ffn_norm(x)  # line 8\n",
    "        # x shape (batch_size, sequence_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ad9a906f-fb4d-4317-8503-923e10b3e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.attention_output_size = d_model // num_heads\n",
    "        # 64 = 512 // 8\n",
    "      \n",
    "        # H layers of attentions,veach attention layer h\n",
    "        # consists of the following params\n",
    "        # (formula 4 and alg 5 from [2]):\n",
    "        # W_q^h, b_q^h query params\n",
    "        # W_k^h, b_k^h key params\n",
    "        # W_v^h, b_v^h value params\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                SelfAttention(d_model, self.attention_output_size)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # W_O in formula (4) and alg 5 from [2]\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        # Output of each layer is concatenated.\n",
    "        # line 3 of alg 5 from [2]\n",
    "        # Y_h shape (n, m) (fill in n, m)!!!!!\n",
    "        # Y_1, Y_h, ..., Y_H -> Y shape (n, h * m)\n",
    "        # q, k, v shape (batch_size, sequence_len, d_model)\n",
    "        x = torch.cat(\n",
    "            [\n",
    "                # line 1, 2 of alg 5 from [2]\n",
    "                layer(q, k, v, mask) for layer in self.attentions\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        \n",
    "        # Final dense layer after the attentions.\n",
    "        # shape (batch_size, sequence_len, d_model)\n",
    "        # 8 heads each producing shape (batch_size, sequence_len, 64)\n",
    "        # concatenated into (batch_size, sequence_len, 512 == 8 * 64)\n",
    "        x = self.output(x)\n",
    "        # shape (batch_size, sequence_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c4b4203-f41d-4ea5-9550-9f46199d1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    # Self attention -> attention on the same sequence (X = Z)\n",
    "    # It tells us which other tokens from the sequence are relevant\n",
    "    # to current token.\n",
    "    \n",
    "    def __init__(self, d_model, output_size, dropout=0.3):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # Params of alg 4 from [2]\n",
    "        # W_q, b_q query params\n",
    "        # W_k, b_k key params\n",
    "        # W_v, b_v value params\n",
    "        self.query = nn.Linear(d_model, output_size)\n",
    "        self.key = nn.Linear(d_model, output_size)\n",
    "        self.value = nn.Linear(d_model, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        query = self.query(q)\n",
    "        key = self.key(k)\n",
    "        value = self.value(v)\n",
    "        # q, k, v are the same tensors in case of self-attention\n",
    "        \n",
    "        batch_size = q.shape[0]\n",
    "        # target and sequence length is the same\n",
    "        # in case of self-attention\n",
    "        target_len = q.shape[1]\n",
    "        sequence_len = k.shape[1]\n",
    "        \n",
    "        dim_k = key.size(-1)\n",
    "        \n",
    "        # batch matrix multiplication\n",
    "        # (b, n, m) x (b, m, p) -> (b, n, p) where b is the size of the batch\n",
    "        # mat_1_i x mat_2_i = mat_out_i where for i-th matrix pair in the batch\n",
    "        # S / sqrt(d_attn) in line 6 of alg 4 from  [2] (S = QK)\n",
    "        scores = torch.bmm(query, key.transpose(1, 2)) / np.sqrt(dim_k)\n",
    "        # scores shape (batch_size, sequence_len, sequence_len)\n",
    "        # query, key shape (batch_size, sequence_len, 512 // 8)\n",
    "        # key.transpose(1, 2) shape (batch_size, 512 // 8, sequence_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # mask shape is (batch_size, sequence_len)\n",
    "            # expanded mask shape is (batch_size, target_len, sequence_len)\n",
    "            expanded_mask = mask[:, None, :].expand(batch_size, target_len, sequence_len)\n",
    "            # triu -> upper triangular part of the matrix. Values below the diagonal of the\n",
    "            # input tensor are set to 0. This creates the unidirectional self-attention.\n",
    "            # Only the current token or the tokens preceeding the current token are considered\n",
    "            # as the context. The model does not look into the future.\n",
    "            subsequent_mask = 1 - torch.triu(\n",
    "                torch.ones((target_len, target_len), device=mask.device, dtype=torch.unit8),\n",
    "                # diagonal=1 means we are including the current token in the context\n",
    "                diagonal=1\n",
    "            )\n",
    "\n",
    "            # Replaces a score with -inf if the mask in the same position is equal to 0.\n",
    "            # line 5 of alg 4 from [2]\n",
    "            scores = scores.masked_fill(expanded_mask == 0, -float(\"inf\"))\n",
    "            scores = scores.masked_fill(subsequent_mask == 0, -float(\"int\"))\n",
    "            \n",
    "        # applying softmax row-wise, line 6 of alg 4 from [2]        \n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # weights.shape == scores.shape\n",
    "        # V^~ = V * softmax(S / sqrt(d_attn))\n",
    "        # V^~ = V * weights, line 6 of alg 4 from [2]\n",
    "        # value shape (batch_size, sequence_len, 512 // 8)\n",
    "        # weight shape (batch_size, sequence_len, sequence_len)\n",
    "        # bmm(weights, value) shape (batch_size, sequence_len, 512 // 8)\n",
    "        return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "608285f9-e68d-4863-9dcd-e6c1e742e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_dim, max_sequence_len):\n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            vocabulary_size, embedding_dim\n",
    "        )\n",
    "        \n",
    "        # alg 2 from [2]\n",
    "        # We are using learned positional embeddings.\n",
    "        # Example https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # is using hard-coded positional embeddings (similar to the sin/cos formula from [2]).\n",
    "        self.positional_embedding = nn.Embedding(\n",
    "            max_sequence_len, embedding_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input: shape (batch_size, sequence_len)\n",
    "        # x = self.embeddings(x)\n",
    "        # there is an embedding vector for every token in every sequence\n",
    "        # output shape (batch_size, sequence_len, embedding_dim)\n",
    "        \n",
    "        batch_size, sequence_len = x.shape\n",
    "        positions = torch.arange(sequence_len, dtype=torch.int32).repeat(batch_size, 1)\n",
    "        \n",
    "        return self.embedding(x) + self.positional_embedding(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26736373-bf2e-471a-9219-a39c32f2b0e0",
   "metadata": {},
   "source": [
    "#### BERT (ETransformer)\n",
    "Based on the `Encoder` from the `EDTransformer` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ed0b35d1-9d21-448a-95d3-f20fad53dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=256,\n",
    "        num_heads=2,\n",
    "        num_encoders=2,\n",
    "        vocabulary_size=100_000,\n",
    "        max_sequence_len=128,\n",
    "    ):\n",
    "        super(ETransformer, self).__init__()\n",
    "        \n",
    "        # Add an id for the mask out token (used during the training).\n",
    "        # See \"Encoder-only transformer: BERT\" paragraph in the section 6 of [2].\n",
    "        vocabulary_size += 1\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "        embedding_dim = d_model\n",
    "        \n",
    "        self.embedding = Embedding(vocabulary_size, embedding_dim, max_sequence_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, num_encoders)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, d_model)\n",
    "        )\n",
    "    \n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # We want the final output to be softmax probability for every\n",
    "        # token in the vocabulary. output_size == vocabulary_size\n",
    "        self.unembedding = nn.Linear(d_model, vocabulary_size)\n",
    "    \n",
    "    def forward(self, tokenized_source):\n",
    "        # source = X in [2] (in case of ETransformer)\n",
    "        # source_mask = None because ETransformer is always using mask = 1 everywhere.\n",
    "        # tokenized_source shape (batch_size, sequence_len), rows of sequences\n",
    "        source = self.embedding(tokenized_source)\n",
    "        # source shape (batch_size, sequence_len, embedding_dim)\n",
    "        # each token in each sequence embedded into a vector\n",
    "        # lines 4-9 of alg 9 from [2]\n",
    "        output = self.encoder(source, source_mask=None)\n",
    "        # encoder_output shape (batch_size, sequence_len, d_model == embedding_dim)\n",
    "        # There is an additional dense layer + norm (line 10 of alg 9 from [2], no x +).\n",
    "        output = self.ffn(output)\n",
    "        # output shape (batch_size, sequence_len, d_model)\n",
    "        output = self.ffn_norm(output)\n",
    "        output = self.unembedding(output)\n",
    "        # output shape (batch_size, sequence_len, vocabulary_size)\n",
    "        # Because we are predicting probability that the i-th token of the j-th\n",
    "        # is the k-th entry in the vocabulary.\n",
    "        # We are not applying softmax because the the cross-entropy loss requires\n",
    "        # raw scores https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5d7a2862-04f0-4e13-8525-9190de0699c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_to_flat(dataset):\n",
    "    # Creates one lond 1-D tensor containing the whole dataset as a long \"tokenized\" string.\n",
    "    return torch.cat(\n",
    "        [\n",
    "            torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
    "            for item in dataset\n",
    "            if len(item) > 0\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def create_sequences(flat_data, sequence_len=128):\n",
    "    n_sequences = flat_data.size(0) // sequence_len\n",
    "    \n",
    "    # trim data to fit and reshape\n",
    "    return flat_data[:n_sequences * sequence_len].reshape(n_sequences, sequence_len)\n",
    "\n",
    "class SequenceBatchGenerator:\n",
    "    \n",
    "    def __init__(self, sequences, batch_size):\n",
    "        self._sequences = sequences\n",
    "        self._batch_size = batch_size\n",
    "        self._n_batches = self._sequences.size(0) // batch_size    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._n_batches      \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(self._n_batches):\n",
    "            start = i * self._batch_size\n",
    "            end = start + self._batch_size\n",
    "            yield i, self._sequences[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b388f41-8c9e-4298-99ab-ff9cecf224d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiText2(split=\"test\")\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, dataset), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a73c5da1-9efb-4271-977d-15f1f6cb597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(sequence_generator, model, loss_fn, optimizer, p_mask=0.05):\n",
    "    size = len(sequence_generator._sequences)\n",
    "    for i, sequence_batch in sequence_generator:\n",
    "        # sequence batch := tensor with containing a batch of tokenized sequences.\n",
    "        # sequence batch shape (batch_size, sequence_len)\n",
    "\n",
    "        # Deletes random tokens from the sequences with the probability p_mask.\n",
    "        # Replaces them with the masked_token_id. Is that correct? What should be the replacement?\n",
    "        # line 5 of alg 12 and section 6 from [2]\n",
    "        mask = torch.rand(*sequence_batch.shape) < p_mask\n",
    "        masked_sequence_batch = sequence_batch.clone()\n",
    "\n",
    "        # Defines a special id for the masked out token.\n",
    "        # We added +1 in the model.__init__ to the vocabulary_size\n",
    "        # to make room for the masked out token.\n",
    "        # max token id == vocab length - 1\n",
    "        masked_token_id = model.vocabulary_size - 1\n",
    "        masked_sequence_batch[mask] = masked_token_id\n",
    "\n",
    "        # y_hat shape (batch_size, sequence_len, vocabulary_size)\n",
    "        y_hat = model(masked_sequence_batch)\n",
    "\n",
    "        # This will one-hot encodes the token ids (entries in the tensor)\n",
    "        # which reshapes tensor\n",
    "        # from (batch_size, sequence_len)\n",
    "        # to (batch_size, sequence_len, vocabulary_size).\n",
    "        # We are using the unmasked sequences with the original token ids as the target.\n",
    "        y = F.one_hot(sequence_batch, num_classes=model.vocabulary_size).float()\n",
    "        \n",
    "        # To compute the loss, we use only only the probabilities\n",
    "        # of the masked out tokens being the original token.\n",
    "        # That's line 7 of alg 12 from [2].\n",
    "        # mask shape (batch_size, sequence_len)\n",
    "        # Applying a 2-d mask to a 3-d tensor flattens the output tensor to 2-d.\n",
    "        # y_hat[mask] shape (number of masked out tokens, vocabulary_size)\n",
    "        # y[mask] shape (number of masked out token, vocabulary_size)\n",
    "        \n",
    "        loss = loss_fn(y_hat[mask], y[mask])\n",
    "        \n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        # also allows target that is not one-hot encoded.\n",
    "        # loss = loss_fn(y_hat[mask], sequence_batch[mask]) should work the same\n",
    "        # y_hat[mask] shape (number of masked out tokens, vocabulary_size)\n",
    "        # 2-d mask applied to 2-d tensor -> 1-d tensor\n",
    "        # sequence_batch[mask] shape (number of masked out tokens,)\n",
    "        \n",
    "        # backprop\n",
    "        # reset param gradients at the start of each iteration\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            loss_, current = loss.item(), i * len(sequence_batch)\n",
    "            print(f\"loss: {loss_:.7f}, [{current}/{size}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ddc20672-d545-4cc2-b44f-f20c5f2a0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = create_sequences(process_to_flat(dataset))\n",
    "sequence_generator = SequenceBatchGenerator(sequences, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a5b68bf6-f188-4146-9a3a-ce8f4c5145fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "loss: 9.5944529, [0/1889]\n",
      "loss: 9.5125141, [320/1889]\n",
      "loss: 9.4790668, [640/1889]\n",
      "loss: 9.4142036, [960/1889]\n",
      "loss: 9.2668324, [1280/1889]\n",
      "loss: 9.1779461, [1600/1889]\n",
      "Epoch: 1\n",
      "loss: 9.2368364, [0/1889]\n",
      "loss: 9.0440292, [320/1889]\n",
      "loss: 9.0519724, [640/1889]\n",
      "loss: 9.0298853, [960/1889]\n",
      "loss: 8.8501883, [1280/1889]\n",
      "loss: 8.8370934, [1600/1889]\n"
     ]
    }
   ],
   "source": [
    "model = ETransformer(vocabulary_size=len(vocab))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "epochs = 2\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch: {i}\")\n",
    "    train_one_epoch(sequence_generator, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a4b6c686-4588-4c02-b26e-e38c0167c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = 5\n",
    "x = torch.Tensor([[0, 2, 1], [3, 4, 4]]).long()\n",
    "\n",
    "masked_x = x.clone()\n",
    "mask = torch.Tensor([[True, True, True], [False, True, True]]).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "86e784d0-fe80-4181-a67c-f424a7408c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 4, 4])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[mask]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
