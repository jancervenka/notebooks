{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "97819d9c-4e56-49b6-9b6c-c40387731ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "import tqdm.notebook as tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be6c6d6-e32d-415f-ade4-064fc9c834e2",
   "metadata": {},
   "source": [
    "## LLM Tokenization\n",
    "https://en.wikipedia.org/wiki/Byte_pair_encoding#Modified_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dfd73331-c6a9-428a-b5e3-ae9ee818af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3f1e94c0-565a-4821-ba1f-dfcef98847e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncodingTokenizer:\n",
    "    def __init__(self, vocab_size: int = 512):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokens_to_byte_tuples = {}\n",
    "\n",
    "    def _vocab_size_reached(self) -> bool:\n",
    "        return len(self.tokens_to_byte_tuples) == self.vocab_size\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_most_common_byte_tuple_pair(byte_tuples: list[tuple[int]]) -> tuple[tuple[int]]:\n",
    "        pairs = []\n",
    "        for i in range(len(byte_tuples) - 1):\n",
    "            first, second = byte_tuples[i], byte_tuples[i + 1]\n",
    "            pairs.append((first, second))\n",
    "\n",
    "        most_common = Counter(pairs).most_common(n=1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_most_common_byte_tuple_pair(\n",
    "        byte_tuples: list[tuple[int]],\n",
    "        most_common_byte_tuple_pair: tuple[tuple[int]],\n",
    "    ) -> list[tuple[int]]:\n",
    "        new_byte_tuples = []\n",
    "        i = 0\n",
    "        while i < len(byte_tuples) - 1:\n",
    "            first, second  = byte_tuples[i], byte_tuples[i + 1]\n",
    "            if (first, second) == most_common_byte_tuple_pair:\n",
    "                new_byte_tuples.append(first + second)\n",
    "                i += 2                \n",
    "            else:\n",
    "                new_byte_tuples.append(first)\n",
    "                i += 1\n",
    "        return new_byte_tuples\n",
    "\n",
    "    def fill_missing_bytes(self) -> None:\n",
    "        # allows to encode characters that weren't in the training data\n",
    "        max_token = max(self.tokens_to_byte_tuples.keys())\n",
    "        for byte_ in range(256): \n",
    "            if (byte_,) not in self.tokens_to_byte_tuples.values():\n",
    "                max_token += 1\n",
    "                self.tokens_to_byte_tuples[max_token] = (byte_,)\n",
    "    \n",
    "    def fit(self, text: str) -> None:\n",
    "        # text as a list of utf8 bytes = [29, 19, 255, ...]\n",
    "        btext = list(text.encode(\"utf8\"))\n",
    "        # each token consists of tuple of bytes\n",
    "        # starting with 1 tuple = 1 byte\n",
    "        byte_tuples = [(byte_,) for byte_ in btext]\n",
    "        while not self._vocab_size_reached():\n",
    "            most_common_byte_tuple_pair = self._find_most_common_byte_tuple_pair(byte_tuples)\n",
    "            byte_tuples = self._merge_most_common_byte_tuple_pair(byte_tuples, most_common_byte_tuple_pair)\n",
    "            unique_byte_tuples = list(set(byte_tuples))\n",
    "            self.tokens_to_byte_tuples = {i: byte_tuple for i, byte_tuple in enumerate(unique_byte_tuples)}\n",
    "\n",
    "        self.fill_missing_bytes()\n",
    "\n",
    "    def get_sorted_tokens_to_byte_tuples(self) -> list[int, tuple[int]]:\n",
    "        return (sorted(self.tokens_to_byte_tuples.items(), key=lambda i: len(i[1])))[::-1]\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "\n",
    "        bytes_ = list(text.encode(\"utf8\"))\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(bytes_):\n",
    "            for token, byte_tuple in self.get_sorted_tokens_to_byte_tuples():\n",
    "                tuple_len = len(byte_tuple)\n",
    "                if byte_tuple == tuple(bytes_[i:i + tuple_len]):\n",
    "                    tokens.append(token)\n",
    "                    i += tuple_len\n",
    "                    break\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        byte_tuples = [self.tokens_to_byte_tuples[token] for token in tokens]\n",
    "        bytes_ = [byte_ for byte_tuple in byte_tuples for byte_ in byte_tuple]\n",
    "        return bytes(bytes_).decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6ba4e35a-55c9-4704-82c2-bc98e04eafda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae3d118679643d994ee1655229ff42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test():\n",
    "    bpe = BytePairEncodingTokenizer(vocab_size=256)\n",
    "    small_text = text[:10000] \n",
    "    bpe.fit(small_text)\n",
    "\n",
    "    segment_size = 512\n",
    "    for _ in tqdm.tqdm(range(200)):\n",
    "        start = random.randint(0, len(small_text) - segment_size)\n",
    "        text_to_encode = small_text[start:start + segment_size]\n",
    "        assert bpe.decode(bpe.encode(text_to_encode)) == text_to_encode\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "90ba483a-48fb-4c84-997a-101bb093df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BytePairEncodingTokenizer(vocab_size=256)\n",
    "small_text = text[:10000] \n",
    "bpe.fit(small_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c09d7c16-4f8f-4249-95b7-a589bb58b185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[204, 101, 41, 143, 87, 200, 30, 110, 188, 102, 15, 10, 141, 230]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.encode(\"Before we proceed any further\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5beaf024-431c-45e8-9320-d1b26753815a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Before we proceed any further'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.decode(bpe.encode(\"Before we proceed any further\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f3747794-366e-4ac6-8ab6-eeb93203d1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'čšáuaweada'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.decode(bpe.encode(\"čšáuaweada\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2e8e2ef3-c3ef-4229-9cb0-132e7f3672b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[397, 342, 398, 362, 396, 362, 149, 58, 177, 6, 253, 58]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.encode(\"čšáuaweada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
