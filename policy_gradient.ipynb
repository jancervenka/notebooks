{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec1919d-686c-4f03-9dc6-42cd7a60ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dbfaedb-ad2d-405e-a185-d11bfce891a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    # feed forward network\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers.append(nn.Linear(sizes[j], sizes[j + 1]))\n",
    "        layers.append(act())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "26d93572-cdf1-44b9-969c-14023ebd810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env=\"CartPole-v0\",\n",
    "    hidden_sizes=[32],\n",
    "    lr=1e-02,\n",
    "    epochs=50,\n",
    "    batch_size=5000,\n",
    "    render=False,\n",
    "    use_reward_to_go=False,\n",
    "):\n",
    "    env = gym.make(env)\n",
    "    assert isinstance(env.observation_space, Box), \"This example works only for envs with continuous state spaces\"\n",
    "    assert isinstance(env.action_space, Discrete), \"This example works only for envs with discrete action space\"\n",
    "\n",
    "    observation_dims = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    " \n",
    "    logits_net = mlp(sizes=[observation_dims] + hidden_sizes + [n_actions])\n",
    "\n",
    "    def reward_to_go(rewards):\n",
    "        n = len(rewards)\n",
    "        rtgs = np.zeros_like(rewards)\n",
    "        for i in reversed(range(n)):\n",
    "            rtgs[i] = rewards[i] + (rtgs[i + 1] if i + 1 < n else 0)\n",
    "        return rtgs\n",
    "\n",
    "    # creates an action probability distribution \\pi(a|s) - categorical because discrete action space\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        # logits can be passed as logits to nn.Categorical and don't have to be softmaxed\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    # get an action by sampling from policy \\pi(a|s)\n",
    "    # only one observation at a time, only one action returned at a time\n",
    "    # .item() is used to retrieve a value from Tensor contaning a single value\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    # when weights are equal to episode returns -> loss gradient = policy gradient\n",
    "    # see the equation for sample policy gradient https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\n",
    "    # !! it's not an acutall loss function from supervised training\n",
    "    def compute_loss(obs, actions, weights):\n",
    "        # get the log probability of an action which is drawn from a policy `get_policy(obs)`\n",
    "        logp = get_policy(obs).log_prob(actions)\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
    "    \n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_actions = []      # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_returns = []      # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "    \n",
    "        # reset episode-specific variables\n",
    "        obs, _ = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rewards = []         # list for rewards accrued throughout episode\n",
    "\n",
    "        finished_rendering_this_epoch = False\n",
    "        # collect experience by acting in the environment with the current policy\n",
    "\n",
    "        while True:\n",
    "    \n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "    \n",
    "            # save observations\n",
    "            batch_obs.append(obs.copy())\n",
    "            # act in the environment\n",
    "            action = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "            batch_actions.append(action)\n",
    "            ep_rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about the episode\n",
    "                ep_return, ep_len = sum(ep_rewards), len(ep_rewards)\n",
    "                batch_returns.append(ep_return)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                if use_reward_to_go:\n",
    "                    batch_weights += list(reward_to_go(ep_rewards))\n",
    "                else:\n",
    "                    batch_weights += [ep_return] * ep_len\n",
    "\n",
    "                # reset episode specific variables\n",
    "                (obs, _), done, ep_rewards = env.reset(), False, []\n",
    "                # wont render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # print(batch_obs)\n",
    "                # print(batch_actions)\n",
    "                # print(batch_weights)\n",
    "                # print(len(batch_obs), len(batch_actions), len(batch_weights))\n",
    "                # raise Exception()\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "    \n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(\n",
    "            obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "            actions=torch.as_tensor(batch_actions, dtype=torch.int32),\n",
    "            weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "        )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_returns, batch_lens\n",
    "\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_returns, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_returns), np.mean(batch_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e667bcaf-408c-4789-a1d4-da7a4ee250d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 18.793 \t return: 22.545 \t ep_len: 22.545\n",
      "epoch:   1 \t loss: 27.058 \t return: 30.311 \t ep_len: 30.311\n",
      "epoch:   2 \t loss: 24.918 \t return: 29.645 \t ep_len: 29.645\n",
      "epoch:   3 \t loss: 29.556 \t return: 36.065 \t ep_len: 36.065\n",
      "epoch:   4 \t loss: 33.602 \t return: 41.397 \t ep_len: 41.397\n",
      "epoch:   5 \t loss: 37.174 \t return: 44.561 \t ep_len: 44.561\n",
      "epoch:   6 \t loss: 36.712 \t return: 47.358 \t ep_len: 47.358\n",
      "epoch:   7 \t loss: 39.574 \t return: 51.051 \t ep_len: 51.051\n",
      "epoch:   8 \t loss: 40.138 \t return: 53.213 \t ep_len: 53.213\n",
      "epoch:   9 \t loss: 39.717 \t return: 52.758 \t ep_len: 52.758\n",
      "epoch:  10 \t loss: 43.056 \t return: 56.539 \t ep_len: 56.539\n",
      "epoch:  11 \t loss: 40.463 \t return: 56.088 \t ep_len: 56.088\n",
      "epoch:  12 \t loss: 40.984 \t return: 59.140 \t ep_len: 59.140\n",
      "epoch:  13 \t loss: 41.284 \t return: 58.419 \t ep_len: 58.419\n",
      "epoch:  14 \t loss: 39.991 \t return: 56.629 \t ep_len: 56.629\n",
      "epoch:  15 \t loss: 39.541 \t return: 56.989 \t ep_len: 56.989\n",
      "epoch:  16 \t loss: 44.369 \t return: 63.975 \t ep_len: 63.975\n",
      "epoch:  17 \t loss: 54.996 \t return: 73.478 \t ep_len: 73.478\n",
      "epoch:  18 \t loss: 49.438 \t return: 73.014 \t ep_len: 73.014\n",
      "epoch:  19 \t loss: 62.469 \t return: 88.772 \t ep_len: 88.772\n",
      "epoch:  20 \t loss: 70.455 \t return: 102.531 \t ep_len: 102.531\n",
      "epoch:  21 \t loss: 71.118 \t return: 104.375 \t ep_len: 104.375\n",
      "epoch:  22 \t loss: 70.690 \t return: 104.082 \t ep_len: 104.082\n",
      "epoch:  23 \t loss: 82.404 \t return: 114.091 \t ep_len: 114.091\n",
      "epoch:  24 \t loss: 93.974 \t return: 134.579 \t ep_len: 134.579\n",
      "epoch:  25 \t loss: 96.104 \t return: 140.444 \t ep_len: 140.444\n",
      "epoch:  26 \t loss: 211.935 \t return: 239.571 \t ep_len: 239.571\n",
      "epoch:  27 \t loss: 195.791 \t return: 265.842 \t ep_len: 265.842\n",
      "epoch:  28 \t loss: 184.203 \t return: 270.053 \t ep_len: 270.053\n",
      "epoch:  29 \t loss: 227.937 \t return: 296.529 \t ep_len: 296.529\n",
      "epoch:  30 \t loss: 318.286 \t return: 367.714 \t ep_len: 367.714\n",
      "epoch:  31 \t loss: 257.628 \t return: 365.929 \t ep_len: 365.929\n",
      "epoch:  32 \t loss: 368.969 \t return: 474.273 \t ep_len: 474.273\n",
      "epoch:  33 \t loss: 303.767 \t return: 421.833 \t ep_len: 421.833\n",
      "epoch:  34 \t loss: 457.991 \t return: 567.667 \t ep_len: 567.667\n",
      "epoch:  35 \t loss: 561.708 \t return: 518.600 \t ep_len: 518.600\n",
      "epoch:  36 \t loss: 419.471 \t return: 417.917 \t ep_len: 417.917\n",
      "epoch:  37 \t loss: 324.770 \t return: 335.938 \t ep_len: 335.938\n",
      "epoch:  38 \t loss: 271.407 \t return: 362.133 \t ep_len: 362.133\n",
      "epoch:  39 \t loss: 291.606 \t return: 335.200 \t ep_len: 335.200\n",
      "epoch:  40 \t loss: 277.689 \t return: 387.846 \t ep_len: 387.846\n",
      "epoch:  41 \t loss: 201.263 \t return: 310.941 \t ep_len: 310.941\n",
      "epoch:  42 \t loss: 178.368 \t return: 280.000 \t ep_len: 280.000\n",
      "epoch:  43 \t loss: 150.621 \t return: 248.048 \t ep_len: 248.048\n",
      "epoch:  44 \t loss: 142.425 \t return: 241.190 \t ep_len: 241.190\n",
      "epoch:  45 \t loss: 131.080 \t return: 212.375 \t ep_len: 212.375\n",
      "epoch:  46 \t loss: 125.080 \t return: 212.458 \t ep_len: 212.458\n",
      "epoch:  47 \t loss: 124.238 \t return: 210.958 \t ep_len: 210.958\n",
      "epoch:  48 \t loss: 124.565 \t return: 208.375 \t ep_len: 208.375\n",
      "epoch:  49 \t loss: 121.416 \t return: 208.000 \t ep_len: 208.000\n"
     ]
    }
   ],
   "source": [
    "train(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "346ee383-ad52-4a5c-b5e6-35e63a7056ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 10.597 \t return: 22.484 \t ep_len: 22.484\n",
      "epoch:   1 \t loss: 13.204 \t return: 27.794 \t ep_len: 27.794\n",
      "epoch:   2 \t loss: 11.977 \t return: 26.898 \t ep_len: 26.898\n",
      "epoch:   3 \t loss: 14.334 \t return: 31.879 \t ep_len: 31.879\n",
      "epoch:   4 \t loss: 14.291 \t return: 32.172 \t ep_len: 32.172\n",
      "epoch:   5 \t loss: 18.655 \t return: 40.516 \t ep_len: 40.516\n",
      "epoch:   6 \t loss: 16.793 \t return: 39.016 \t ep_len: 39.016\n",
      "epoch:   7 \t loss: 19.588 \t return: 44.804 \t ep_len: 44.804\n",
      "epoch:   8 \t loss: 20.537 \t return: 46.343 \t ep_len: 46.343\n",
      "epoch:   9 \t loss: 21.589 \t return: 52.031 \t ep_len: 52.031\n",
      "epoch:  10 \t loss: 20.967 \t return: 52.188 \t ep_len: 52.188\n",
      "epoch:  11 \t loss: 20.233 \t return: 53.564 \t ep_len: 53.564\n",
      "epoch:  12 \t loss: 23.753 \t return: 63.175 \t ep_len: 63.175\n",
      "epoch:  13 \t loss: 27.168 \t return: 70.620 \t ep_len: 70.620\n",
      "epoch:  14 \t loss: 30.306 \t return: 79.429 \t ep_len: 79.429\n",
      "epoch:  15 \t loss: 30.020 \t return: 84.017 \t ep_len: 84.017\n",
      "epoch:  16 \t loss: 32.824 \t return: 83.557 \t ep_len: 83.557\n",
      "epoch:  17 \t loss: 29.460 \t return: 81.210 \t ep_len: 81.210\n",
      "epoch:  18 \t loss: 42.147 \t return: 109.587 \t ep_len: 109.587\n",
      "epoch:  19 \t loss: 40.737 \t return: 119.905 \t ep_len: 119.905\n",
      "epoch:  20 \t loss: 62.524 \t return: 153.706 \t ep_len: 153.706\n",
      "epoch:  21 \t loss: 70.400 \t return: 197.370 \t ep_len: 197.370\n",
      "epoch:  22 \t loss: 70.101 \t return: 194.423 \t ep_len: 194.423\n",
      "epoch:  23 \t loss: 77.655 \t return: 202.231 \t ep_len: 202.231\n",
      "epoch:  24 \t loss: 71.235 \t return: 187.893 \t ep_len: 187.893\n",
      "epoch:  25 \t loss: 80.788 \t return: 218.000 \t ep_len: 218.000\n",
      "epoch:  26 \t loss: 68.952 \t return: 202.600 \t ep_len: 202.600\n",
      "epoch:  27 \t loss: 86.911 \t return: 252.300 \t ep_len: 252.300\n",
      "epoch:  28 \t loss: 95.172 \t return: 268.650 \t ep_len: 268.650\n",
      "epoch:  29 \t loss: 114.804 \t return: 356.800 \t ep_len: 356.800\n",
      "epoch:  30 \t loss: 127.540 \t return: 392.231 \t ep_len: 392.231\n",
      "epoch:  31 \t loss: 120.382 \t return: 380.357 \t ep_len: 380.357\n",
      "epoch:  32 \t loss: 146.916 \t return: 420.667 \t ep_len: 420.667\n",
      "epoch:  33 \t loss: 171.759 \t return: 486.364 \t ep_len: 486.364\n",
      "epoch:  34 \t loss: 129.007 \t return: 438.250 \t ep_len: 438.250\n",
      "epoch:  35 \t loss: 146.980 \t return: 454.250 \t ep_len: 454.250\n",
      "epoch:  36 \t loss: 156.053 \t return: 506.000 \t ep_len: 506.000\n",
      "epoch:  37 \t loss: 153.176 \t return: 463.909 \t ep_len: 463.909\n",
      "epoch:  38 \t loss: 326.308 \t return: 896.833 \t ep_len: 896.833\n",
      "epoch:  39 \t loss: 310.422 \t return: 898.500 \t ep_len: 898.500\n",
      "epoch:  40 \t loss: 359.131 \t return: 1173.600 \t ep_len: 1173.600\n",
      "epoch:  41 \t loss: 524.463 \t return: 1278.500 \t ep_len: 1278.500\n",
      "epoch:  42 \t loss: 563.392 \t return: 1982.000 \t ep_len: 1982.000\n",
      "epoch:  43 \t loss: 1105.104 \t return: 4046.000 \t ep_len: 4046.000\n",
      "epoch:  44 \t loss: 1031.518 \t return: 2809.500 \t ep_len: 2809.500\n",
      "epoch:  45 \t loss: 538.317 \t return: 1789.500 \t ep_len: 1789.500\n",
      "epoch:  46 \t loss: 2528.145 \t return: 5478.000 \t ep_len: 5478.000\n",
      "epoch:  47 \t loss: 1767.757 \t return: 6753.000 \t ep_len: 6753.000\n",
      "epoch:  48 \t loss: 691.393 \t return: 2688.000 \t ep_len: 2688.000\n",
      "epoch:  49 \t loss: 199.195 \t return: 747.571 \t ep_len: 747.571\n"
     ]
    }
   ],
   "source": [
    "train(use_reward_to_go=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5d96bd64-d60b-4fba-a9e0-3bc78c9f4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(obs):\n",
    "    # logits can be passed as logits to nn.Categorical and don't have to be softmaxed\n",
    "    return Categorical(logits=torch.as_tensor(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8aa22cdf-2a74-405f-a59c-8d82690ccf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.random.randn(20, 5)\n",
    "actions = np.zeros(20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "82309a7e-9eee-4ff4-9fe8-b526dce7f56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_policy(obs).log_prob(torch.as_tensor(actions)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b2c48-6ef7-4042-b7d3-e95cd9a8f2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
