{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import Input\n",
    "from keras.backend import cast_to_floatx\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.engine import Model\n",
    "from keras.layers import Dense, Dropout, regularizers, concatenate\n",
    "from keras.models import load_model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layer\n",
    "\n",
    "Dense implements the operation:\n",
    "\n",
    "```\n",
    "output = activation(dot(input, kernel) + bias)\n",
    "```\n",
    "\n",
    "where `activation` is the element-wise activation function passed as the activation argument,\n",
    "`kernel` is a weights matrix created by the layer, and `bias` is a bias vector created by\n",
    "the layer (only applicable if `use_bias` is `True`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Neuron Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is just a dot product of the weights $\\boldsymbol{w}$ (1D vector) and input $\\boldsymbol{x}$ (1D vector)\n",
    "passed in to the activation function $f$. The weights and input size is $N=2$ in our case. Bias is $b$ (scalar value).\n",
    "\n",
    "$$f(\\boldsymbol{x} \\cdot \\boldsymbol{w} + b) = f\\left(\\sum_{i=0}^{N}w_{i}x_{i} + b \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activation f(x) = x\n",
    "\n",
    "i = Input(shape=(2,), name='main')\n",
    "d = Dense(1)(i)\n",
    "m = Model(i, d)\n",
    "m.layers[1].set_weights([np.array([[1], [1]]), np.array([0])])    \n",
    "m.compile('rmsprop', 'mse')\n",
    "\n",
    "# just a sum of input (weights are ones)\n",
    "x = np.array([[2, 2]])\n",
    "m.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n",
      "[[22.]]\n"
     ]
    }
   ],
   "source": [
    "# activation f(x) = relu(x)\n",
    "\n",
    "i = Input(shape=(2,), name='main')\n",
    "d = Dense(1, activation='relu')(i)\n",
    "m = Model(i, d)\n",
    "m.layers[1].set_weights([np.array([[1], [1]]), np.array([0])])    \n",
    "m.compile('rmsprop', 'mse')\n",
    "\n",
    "# 0, input sum is < 0\n",
    "x = np.array([[-20, 2]])\n",
    "print(m.predict(x))\n",
    "\n",
    "# just a sum of input - weights are ones and the input sum is > 0\n",
    "x = np.array([[20, 2]])\n",
    "print(m.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Neuron Layer\n",
    "\n",
    "Assume an input of shape $N=2$ and a dense layer with two neurons with weights\n",
    "\n",
    "$$\\boldsymbol{W} = \\begin{bmatrix}1 & 2 \\\\ 1 & 2\\end{bmatrix}$$\n",
    "\n",
    "where each row represents one neuron. The bias vector is\n",
    "\n",
    "$$\\boldsymbol{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "and the actiovation function\n",
    "\n",
    "$$f(x) = x.$$\n",
    "\n",
    "The input vector is\n",
    "\n",
    "$$\\boldsymbol{x} = \\begin{bmatrix}2 & 2\\end{bmatrix}.$$\n",
    "\n",
    "Calculating the dot product of the input and one row of the weight matrix and applying the activation function\n",
    "produces one value of the output vector of the layer. Let $\\hat{f}$ denote the element-wise application of the activation function $f$. The output of the layer is then\n",
    "\n",
    "$$\n",
    "\\hat{f}(\\boldsymbol{x} \\cdot \\boldsymbol{W} + \\boldsymbol{b}) =\n",
    "\\begin{bmatrix}\n",
    "    f(\\boldsymbol{x} \\cdot \\boldsymbol{w}_{1} + b_{1}) \\\\\n",
    "    f(\\boldsymbol{x} \\cdot \\boldsymbol{w}_{2} + b_{2})\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    f(2 \\cdot 1 + 2 \\cdot 1 + 0) \\\\\n",
    "    f(2 \\cdot 2 + 2 \\cdot 2 + 0)\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    4 & 8\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where $\\boldsymbol{x}$ is the input vector, $\\boldsymbol{w}_{1}$ and $\\boldsymbol{w}_{2}$ are the weight vectors for the first\n",
    "and second neuron; $b_{1}$ and $b_{2}$ are biases for the first and second neuron.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}\\cdot\\boldsymbol{W} + \\boldsymbol{b}=\n",
    "\\begin{bmatrix}\n",
    "    2 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1 & 2 \\\\\n",
    "    1 & 2\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\ 0\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    4 & 8\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\n",
    "\\hat{f}(\\boldsymbol{x} \\cdot \\boldsymbol{W} + \\boldsymbol{b}) =\n",
    "\\begin{bmatrix}\n",
    "    f(4) & f(8)\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    4 & 8\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 8.]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activation f(x) = x\n",
    "\n",
    "i = Input(shape=(2,), name='main')\n",
    "d = Dense(2)(i)\n",
    "m = Model(i, d)\n",
    "W = np.array([[1, 2], [1, 2]])\n",
    "m.layers[1].set_weights([W, np.array([0, 0])])    \n",
    "m.compile('rmsprop', 'mse')\n",
    "\n",
    "# just a sum of input (weights are ones)\n",
    "x = np.array([[2, 2]])\n",
    "m.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1. -1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "i = [Input(shape=(2,), name='main'), Input(shape=(3,), name='secondary')]\n",
    "c = concatenate(i)\n",
    "m = Model(i, c)\n",
    "m.compile('rmsprop', 'mse')\n",
    "\n",
    "x_main = np.array([[1, 1]])\n",
    "x_secondary = np.array([[-1, -1, -1]])\n",
    "\n",
    "print(m.predict({'main': x_main, 'secondary': x_secondary}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# effects only training?\n",
    "\n",
    "i = Input(shape=(2,), name='main')\n",
    "d = Dropout(1, seed=0)(i)\n",
    "m = Model(i, d)\n",
    "m.compile('rmsprop', 'mse')\n",
    "\n",
    "# just a sum of input (weights are ones)\n",
    "x = np.array([[1] * 2])\n",
    "m.predict(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
