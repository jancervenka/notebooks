{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer"
      ],
      "metadata": {
        "id": "OCc73J7tsooY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Gbcfhf5aqq5K"
      },
      "outputs": [],
      "source": [
        "class ReasoningModelWrapper:\n",
        "    def __init__(self, model_id: str):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=self.device\n",
        "        )\n",
        "\n",
        "    def format_reasoning_prompt(self, user_query: str):\n",
        "        system_prompt = (\n",
        "            \"You are a deep reasoning AI. You must NOT answer immediately. \"\n",
        "            \"First, use a <think></think> block to break down the problem, check for edge cases, \"\n",
        "            \"and verify your logic step-by-step. \"\n",
        "            \"Only after thinking should you provide the final answer.\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_query}\n",
        "        ]\n",
        "\n",
        "        return self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "    def generate_reasoning(self, user_query: str) -> str:\n",
        "        input_text = self.format_reasoning_prompt(user_query)\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # We start the generation with the opening tag to 'prime' the model\n",
        "        # This forces it into the thinking state immediately.\n",
        "        forced_prefix = \"<think>\\n\"\n",
        "        input_ids = inputs.input_ids\n",
        "        prefix_ids = (\n",
        "            self.tokenizer(forced_prefix, add_special_tokens=False, return_tensors=\"pt\")\n",
        "            .input_ids\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        # Concatenate prompt + forced prefix\n",
        "        combined_input = torch.cat([input_ids, prefix_ids], dim=1)\n",
        "\n",
        "        print(f\"\\n{'='*20} REASONING TRACE {'='*20}\")\n",
        "        print(forced_prefix, end=\"\", flush=True)\n",
        "\n",
        "        # Streamer allows us to see the text as it generates\n",
        "        streamer = TextStreamer(self.tokenizer, skip_prompt=True)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            combined_input,\n",
        "            max_new_tokens=1024,\n",
        "            temperature=0.7,  # Slightly higher temp helps creative reasoning\n",
        "            do_sample=True,\n",
        "            streamer=streamer,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return decoded_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "reasoner = ReasoningModelWrapper(model_id)\n",
        "question = \"Which is heavier: a pound of lead or a pound of feathers? But wait, the feathers are wet.\"\n",
        "output = reasoner.generate_reasoning(question)\n",
        "\n",
        "final_answer = \"\"\n",
        "print(f\"\\n{'='*20} FINAL OUTPUT {'='*20}\")\n",
        "if \"</think>\" in output:\n",
        "    final_answer = output.split(\"</think>\")[-1].strip()\n",
        "    print(f\"Parsed Answer: {final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUFjmUsWsKqm",
        "outputId": "e558bc3c-0f0d-4eab-b568-5895d08df3eb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Qwen/Qwen2.5-1.5B-Instruct on cuda...\n",
            "\n",
            "==================== REASONING TRACE ====================\n",
            "<think>\n",
            "Let's approach this logically:\n",
            "1. We're comparing two objects that weigh exactly one pound each.\n",
            "2. One object is lead (a dense metal), and the other is wet feathers (which are less dense).\n",
            "3. The question asks which is heavier when considering both factors.\n",
            "4. Lead has a higher density than most materials including water. This means it occupies more space per unit weight compared to the same volume of liquid.\n",
            "5. Wet feathers will displace slightly more water than dry feathers because they absorb some of their own mass in moisture.\n",
            "6. However, if we consider only the weight aspect without regard to displacement effects on buoyancy, the lead would still be heavier by its inherent density difference.\n",
            "7. When considering the overall effect on weight due to the presence of liquid, the feathers might appear slightly lighter under certain conditions but not significantly so unless there's a substantial amount of liquid involved.\n",
            "8. In practical scenarios where we can measure the exact weights of these objects, the lead would always be heavier regardless of whether it's wet or dry.\n",
            "</think>\n",
            "Therefore, the lead would be heavier even when wet.<|im_end|>\n",
            "\n",
            "==================== FINAL OUTPUT ====================\n",
            "Parsed Answer: Therefore, the lead would be heavier even when wet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reasoner.format_reasoning_prompt(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9OUCvxZz_GD",
        "outputId": "2ff91f80-08e2-486a-b1e6-6181741607f1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>system\n",
            "You are a deep reasoning AI. You must NOT answer immediately. First, use a <think></think> block to break down the problem, check for edge cases, and verify your logic step-by-step. Only after thinking should you provide the final answer.<|im_end|>\n",
            "<|im_start|>user\n",
            "Which is heavier: a pound of lead or a pound of feathers? But wait, the feathers are wet.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ]
    }
  ]
}